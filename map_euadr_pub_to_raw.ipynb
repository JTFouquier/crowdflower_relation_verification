{
 "metadata": {
  "name": "",
  "signature": "sha256:232dd7d7312e62bdd5d70c738fd64e6f6c5cc4e6d9c02814b9ee7c24827aca00"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mapping the EU-ADR's published drug-disease relationships back to the raw data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "2015-04-09 Tong Shu Li"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to determine how many expert annotators agreed on any published annotation, the published relationships need to be mapped back to the original raw data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import os\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from src.file_util import read_file\n",
      "from src.get_pubmed_abstract import get_abstract_information"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read the published EU-ADR drug-disease relations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pub_euadr = pd.read_csv(\"data/pub_EUADR_drug_disease_id.tsv\", sep = \"\\t\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def convert(text):\n",
      "    mapping = {\n",
      "        \"PA\": \"positive\",\n",
      "        \"SA\": \"speculative\",\n",
      "        \"NA\": \"negative\",\n",
      "        \"FA\": \"false\"\n",
      "    }\n",
      "    assert text in mapping\n",
      "    return mapping[text]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Every annotation in the EU-ADR has a final boolean stating whether that relationship was true or not, and a computer generated relationship type. If the final decision is \"false\", then the relationship type is also \"false\". When the final decision is \"true\", then the relationship type is the computer extracted relationship type."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def determine_reltype(final, computer):\n",
      "    assert final in [\"True\", \"False\"]\n",
      "    return convert(computer) if final == \"True\" else \"false\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The EU-ADR's raw annotations are stored in individual text files. Each text file represents all of the extracted named entities and relationships for a particular abstract."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function returns a list of all the raw relationships for each abstract, as well as the number of human annotators who agreed with that relationship annotation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_raw_relation_list(pmid):\n",
      "    fname = \"id_{0}.txt\".format(pmid)\n",
      "    loc = \"data/id_raw_euadr_corpus\"\n",
      "    \n",
      "    concepts = dict()\n",
      "    relation = dict()\n",
      "    \n",
      "    for line in read_file(fname, loc):\n",
      "        vals = line.split('\\t')\n",
      "        assert len(vals) in [10, 12]\n",
      "        \n",
      "        if len(vals) == 10:\n",
      "            if vals[1] == \"True\": # a line defining a true concept\n",
      "                # name, start, stop, type\n",
      "                concepts[vals[8]] = (vals[3], int(vals[4]), int(vals[5]), vals[9])\n",
      "        else: # a line defining a relationship\n",
      "            rel_id = vals[11]\n",
      "            \n",
      "            sub = vals[3]\n",
      "            obj = vals[4]\n",
      "            \n",
      "            if sub not in concepts or obj not in concepts:\n",
      "                # some of the raw annotations refer to concepts which do not exist\n",
      "                # example: 17634547_raw_rel_5 is between concepts 16 and 14,\n",
      "                # but concept 14 does not exist\n",
      "                continue\n",
      "            \n",
      "            # keep it in disease, drug order\n",
      "            if concepts[sub][3] == \"Chemicals & Drugs\":\n",
      "                sub, obj = obj, sub\n",
      "                \n",
      "            reltype = determine_reltype(vals[1], vals[10])\n",
      "            \n",
      "            num_agree = len(set(vals[9].split(\",\")) - set([\"Computer\"]))\n",
      "            final = vals[1]\n",
      "            \n",
      "            relation[rel_id] = (reltype, num_agree, concepts[sub], concepts[obj], final)\n",
      "            \n",
      "    return relation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The raw EU-ADR gives the position of each concept with respect to the abstract, while the published EU-ADR gives the position of each concept with respect to the sentence. Therefore we need the original abstract in order to match concepts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concept_match(sent_idx, concept_start, concept_stop, sent_start, sent_stop):\n",
      "    \"\"\"\n",
      "    Determines if a concept from a sentence in the published EU-ADR\n",
      "    matches a concept from the raw EU-ADR by checking the indicies\n",
      "    of the two concepts.\n",
      "    \n",
      "    The raw EU-ADR gives its concept indicies with respect to the abstract,\n",
      "    while the published EU-ADR gives its indicies with respect to the\n",
      "    sentence.\n",
      "    \n",
      "    sent_idx: index of the first letter of the sentence in the abstract\n",
      "    concept_[start, stop]: index of the concept with respect to the abstract\n",
      "    sent_[start, stop]: index of the concept with respect to the sentence\n",
      "    \"\"\"\n",
      "    right_start = concept_start == (sent_idx + sent_start)\n",
      "    right_stop = concept_stop == (sent_idx + sent_stop)\n",
      "    \n",
      "    return right_start and right_stop"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function tries to find an exact match between a specific published EU-ADR association and a raw association. If no exact match is possible, then it returns an arbitrary association that had the maximum number of experts that agreed. Some annotations "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_match(row, raw_relations, abstract):\n",
      "    \"\"\"\n",
      "    Find a match for the published relationship to a raw annotation,\n",
      "    and returns the raw relationship id.\n",
      "    \"\"\"\n",
      "    sentence = row[\"SENTENCE\"]\n",
      "    assert sentence in abstract\n",
      "    \n",
      "    sent_idx = abstract.index(sentence)\n",
      "\n",
      "    # look for a match based on the location of the concepts\n",
      "    matches = set()\n",
      "    for rel_id, info in raw_relations.items():\n",
      "        sub = info[2]\n",
      "        obj = info[3]\n",
      "        \n",
      "        if (concept_match(sent_idx, sub[1], sub[2], row[\"ENTITY1_INI\"], row[\"ENTITY1_END\"])\n",
      "            and concept_match(sent_idx, obj[1], obj[2], row[\"ENTITY2_INI\"], row[\"ENTITY2_END\"])):\n",
      "            matches.add(rel_id)\n",
      "            \n",
      "    if len(matches) == 1:\n",
      "        return list(matches)[0]\n",
      "    \n",
      "    # more than one relationship matches (eg 17541882_raw_rel_2, rel_3)\n",
      "    \n",
      "    if len(matches) > 1:\n",
      "        # try to find the one \"True\" annotation\n",
      "        for rel_id in matches:\n",
      "            if raw_relations[rel_id][4] == \"True\":\n",
      "                return rel_id\n",
      "\n",
      "        # could not find a true annotation; they are all false\n",
      "        \n",
      "        # look for the one with most agreement\n",
      "        most_agree = -1\n",
      "        ans = \"\"\n",
      "        for rel_id in matches:\n",
      "            if raw_relations[rel_id][1] > most_agree:\n",
      "                most_agree = raw_relations[rel_id][1]\n",
      "                ans = rel_id\n",
      "\n",
      "        # 17894421, rel_5 and 11, two diff people both agreed with diff answers for the real ans\n",
      "        # speculative and negative\n",
      "        # since num agree is same, we can just randomly pick one... (figure this out later)\n",
      "        return ans\n",
      "    \n",
      "    # try an exact name match... (desperate measures)\n",
      "    matches = set()\n",
      "    for rel_id, info in raw_relations.items():\n",
      "        sub = info[2]\n",
      "        obj = info[3]\n",
      "        \n",
      "        if sub[0] == row[\"ENTITY1_TEXT\"] and obj[0] == row[\"ENTITY2_TEXT\"]:\n",
      "            matches.add(rel_id)\n",
      "            \n",
      "    assert len(matches) == 1, matches\n",
      "    return list(matches)[0]\n",
      "    \n",
      "    raise Exception(\"no match!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# all sentences can be found directly..\n",
      "mapping = dict()\n",
      "for pmid, group in pub_euadr.groupby(\"PMID\"):\n",
      "    title, chunks = get_abstract_information(str(pmid))\n",
      "    abstract = title + \" \" + \" \".join(chunks)\n",
      "    \n",
      "    raw_relations = get_raw_relation_list(pmid)\n",
      "    \n",
      "    for index, row in group.iterrows():\n",
      "        raw_id = find_match(row, raw_relations, abstract)\n",
      "        mapping[row[\"pub_rel_id\"]] = (raw_id, raw_relations[raw_id][1])\n",
      "        \n",
      "print \"Done\"    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "header = [\"pub_rel_id\", \"raw_relation_id\", \"num_expert_agree\"]\n",
      "with open(\"data/pub_to_raw_mapping.txt\", \"w\") as out:\n",
      "    out.write(\"{0}\\n\".format(\"|\".join(header)))\n",
      "    for toby_id, info in mapping.items():\n",
      "        out.write(\"{0}|{1}|{2}\\n\".format(toby_id, info[0], info[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}